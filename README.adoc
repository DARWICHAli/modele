= Modèles
v1.0 2022-11-16
:doctype: report
:toc: macro
:stem: latexmath
:data-uri:
:toclevels: 3
:toc-title: Table des matières

== Introduction

Ce dépôt défini le code des différents modèles.

Ce fichier commence par expliquer comment tester notre modèle d'annotation via une interface graphique, puis comment installer le module. La suite explique brièvement les entrées/sorties et le fonctionnement global des modèles.

== Tester le modèle

Le site est déployé à l'adresse suivante: https://www.captioning.zefresk.com/ 

L'interface est simple à utiliser. Vous pouvez retrouver une explication detaillée de son fonctionnement dans le dépôt *Interface*: https://git.unistra.fr/image-labellisation/interface

== Installation

=== Dépendances

Ce dépôt est testé avec :

- torch 1.13.1
- opencv 0.14.1
- detectron2 0.2.1

=== Instructions

L'installation du modèle est complexe mais faisable. Elle nécessite d'**installer une version précise de detectron2**.

0. Cloner le dépôt et ses sous-modules.
```bash
git clone --recursive https://git.unistra.fr/image-labellisation/modele.git
```

1. (facultatif) Il est recommandé de créer un environnement virtuel avec venv, puis l'ouvrir :
```bash
python -m venv env
source ./env/bin/activate
```

2. Installer les packages simples à installer
```bash
pip install torch opencv-python pycocotools h5py line_profiler spacy
```

3. Installer le modèle spacy :
```python
python -m spacy download en_core_web_md
```

4. Installer detectron2 v0.2.1
```bash
pip install -e detectron2
```

5. Compiler et installer les modèles `bua` :
```python
python setup.py build develop
```

6. Il reste à télécharger les modèles (voir section suivante).

=== Modèles

Les modèles doivent être téléchargés à part et mis dans le dossier courant.

- Resnet101 entraîné sur Visual Genome. Poids originaux de link:https://github.com/peteanderson80[peteanderson80] de l'article link:https://arxiv.org/abs/1707.07998[Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering] traduit pour detectron2 par l'équipe du MIT link:https://github.com/MILVLG/bottom-up-attention.pytorch/tree/4dbce869ad17117ca9f1df55bc5604cdbcd47f59[MILVLG] : link:https://seafile.unistra.fr/f/d1bddb9791ff4fc596f3/?dl=1[bua-caffe-frcn-r101-k10-100.pth]
```bash
wget --content-disposition "https://seafile.unistra.fr/f/d1bddb9791ff4fc596f3/?dl=1"
```

- M2 entraîné par link:https://github.com/aimagelab[aimagelab] dans l'article link:https://arxiv.org/abs/1912.08226[Meshed-Memory Transformer for Image Captioning] : https://seafile.unistra.fr/f/7f271666b0874097be6b/?dl=1
```bash
wget --content-disposition "https://seafile.unistra.fr/f/7f271666b0874097be6b/?dl=1"
```

- Tous les modèles nécessaires à l'utilisation de l'API sont disponible ici : link:https://seafile.unistra.fr/f/e0708907f3934c2ba4c5/?dl=1[https://seafile.unistra.fr/f/e0708907f3934c2ba4c5/?dl=1].

== Fonctionnemment global

Le modèle d'annotation peut être découpé en deux parties principales, de manière analogue à un _autoencoder_.

.Structure d'un autoencoder
image::res/encdec.png[Autoencoder]

== Encodeur / Extraction de features

L'"encodeur" d'après les link:https://git.unistra.fr/image-labellisation/ressources/-/blob/main/short/transformer.md[articles étudiés] et ce link:https://github.com/yiren-jian/Bottom-Up-Features-Detectron2[dépôt] prend en entrée des images RGB et retourne un certain nombre de **masques rectangulaires** ainsi que la **représentation latente des pixels dans le masque**.

En effet la partie "décodeur" est composée d'un R-CNN qui a pour objectif d'extraire les ROI(Region Of Interest) les plus "essentielles" à la description de l'image sous forme de "bounding-box". Le nombre de bounding box est décrit par la variable stem:[N]. 
Ensuite, une représentation latente de ces stem:[N] régions va être déterminée à l'aide d'un réseau à convolution "classique" (e.g. ResNet).

Le modèle est donc chargé d'extraire un certain nombre de features mais pas la position de ces dernières (**bruh**).

D'un point de vue tensoriel, si la représentation latente est de dimension stem:[K] :

[stem]
++++
\text{input}(1) = (224, 224, 3) \\
\text{output}(1) = (N, K)
++++

_Oui, le M2 n'utilise pas la position des features ????_

.Traitement d'une image par l'encodeur
image::res/encoder.png[Encodeur pipeline]

Par exemple, si l'encodeur utilise link:https://keras.io/api/applications/resnet/[ResNet] , la dimension de plongement est 2048, donc stem:[K=2048] et la sortie sera de dimension stem:[(N, 2048)].

== Décodeur / Génération d'annotation

Le "décodeur" prend en entrée les stem:[N] features de l'encodeur et utilise un réseau de link:https://git.unistra.fr/image-labellisation/ressources/-/blob/main/short/transformer.md[transformeurs] (et donc un modèle encodeur/décodeur) pour générer les tokens un à un et ainsi former une phrase.
Comme le réseau décodeur est un transformeur, les phrases finales ne pourront pas dépasser une longueur fixe stem:[L].

D'un point de vue tensoriel, si la représentation latente est de dimension stem:[K] :

[stem]
++++
\text{input}(1) = (N, K) \\
\text{output}(1) = (1, L)
++++

.Traitement des features par le décodeur
image::res/decoder.png[Décodeur en action]
